{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "protective-recycling",
   "metadata": {},
   "source": [
    "# Cross Validation to find optimal latent rank (and hyperparamters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "graphic-happening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import numba\n",
    "from lenskit import batch, topn, util\n",
    "from lenskit import crossfold as xf\n",
    "from lenskit.algorithms import Recommender, Predictor, als, basic, user_knn\n",
    "from lenskit import topn\n",
    "from scipy import sparse\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from lenskit.data import sparse_ratings\n",
    "\n",
    "# Options for pandas\n",
    "pd.options.display.max_columns = 50\n",
    "pd.options.display.max_rows = 30\n",
    "\n",
    "# Visualizations and debugging\n",
    "import plotly.graph_objs as go\n",
    "#from pprintpp import pprint as pp\n",
    "import logging\n",
    "\n",
    "# Autoreload extension\n",
    "if 'autoreload' not in get_ipython().extension_manager.loaded:\n",
    "    %load_ext autoreload\n",
    "    \n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "tough-burke",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse.linalg import svds, eigs, norm\n",
    "from scipy.sparse import csr_matrix\n",
    "from numpy import linalg as LA\n",
    "from numba import jit, njit, prange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "corresponding-flower",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset\n",
    "ratings = pd.read_parquet('ratings.parquet.gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-compiler",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "emotional-draft",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nopython=True)\n",
    "def matrix_factorization_pred(X, P, Q, K, steps, alpha, beta, Mask):\n",
    "    #    Mask = (X!=0)\n",
    "    Q = Q.T\n",
    "    error_list = np.zeros(steps+1)\n",
    "    error_list[0] = 100000000\n",
    "    for step in range(steps):\n",
    "        # for each user\n",
    "        for i in prange(X.shape[0]):\n",
    "            # for each item\n",
    "            for j in range(X.shape[1]):\n",
    "                if X[i, j] > 0:\n",
    "\n",
    "                    # calculate the error of the element\n",
    "                    eij = X[i, j] - np.dot(P[i, :], Q[:, j])\n",
    "                    # second norm of P and Q for regularilization\n",
    "                    sum_of_norms = 0\n",
    "                    # for k in xrange(K):\n",
    "                    #    sum_of_norms += LA.norm(P[:,k]) + LA.norm(Q[k,:])\n",
    "                    # added regularized term to the error\n",
    "                    sum_of_norms += LA.norm(P) + LA.norm(Q)\n",
    "                    # print sum_of_norms\n",
    "                    eij += (beta / 2) * sum_of_norms\n",
    "                    # compute the gradient from the error\n",
    "                    for k in range(K):\n",
    "                        P[i][k] = P[i][k] + alpha * (\n",
    "                            2 * eij * Q[k][j] - (beta * P[i][k])\n",
    "                        )\n",
    "                        Q[k][j] = Q[k][j] + alpha * (\n",
    "                            2 * eij * P[i][k] - (beta * Q[k][j])\n",
    "                        )\n",
    "\n",
    "        # compute total error\n",
    "        error = 0\n",
    "        # for each user\n",
    "        extimated_X = np.trunc(P @ Q)\n",
    "        extimated_X = np.where(extimated_X > 5, 5, extimated_X)\n",
    "        extimated_X = np.where(extimated_X < 0, 0, extimated_X)\n",
    "        extimated_error = np.multiply(X - extimated_X, Mask)\n",
    "        err_temp = LA.norm(extimated_error)\n",
    "        error_list[step+1] = err_temp\n",
    "        \n",
    "        print(step)\n",
    "\n",
    "        if np.abs(err_temp - error_list[step]) < 0.001:\n",
    "            break\n",
    "    return extimated_X, P, Q.T, error_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "accredited-corps",
   "metadata": {},
   "outputs": [],
   "source": [
    "# <script src=\"https://gist.github.com/tgsmith61591/ce7d614d7a0442f94cd5ae5d1e51d3c2.js\"></script>\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Author: Taylor G Smith\n",
    "#\n",
    "# More scratch code in my collection of random recommender\n",
    "# system utilities. Someday I'll get around to building\n",
    "# an actual repository... in the meantime, here are some\n",
    "# train/test split utilities for collaborative filtering\n",
    "# with sparse matrices.\n",
    "\n",
    "from __future__ import absolute_import, division\n",
    "\n",
    "import numpy as np\n",
    "from abc import ABCMeta, abstractmethod\n",
    "\n",
    "import six\n",
    "from sklearn.utils.validation import check_random_state\n",
    "from sklearn.utils import validation as skval\n",
    "\n",
    "from scipy import sparse\n",
    "\n",
    "import numbers\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "__all__ = [\n",
    "    'BootstrapCV',\n",
    "    'check_cv',\n",
    "    'train_test_split'\n",
    "]\n",
    "\n",
    "MAX_SEED = 1e6\n",
    "ITYPE = np.int32\n",
    "DTYPE = np.float64  # implicit asks for doubles, not float32s...\n",
    "\n",
    "\n",
    "def check_consistent_length(u, i, r):\n",
    "    \"\"\"Ensure users, items, and ratings are all of the same dimension.\n",
    "    Parameters\n",
    "    ----------\n",
    "    u : array-like, shape=(n_samples,)\n",
    "        A numpy array of the users.\n",
    "    i : array-like, shape=(n_samples,)\n",
    "        A numpy array of the items.\n",
    "    r : array-like, shape=(n_samples,)\n",
    "        A numpy array of the ratings.\n",
    "    \"\"\"\n",
    "    skval.check_consistent_length(u, i, r)\n",
    "    return np.asarray(u), np.asarray(i), np.asarray(r, dtype=DTYPE)\n",
    "\n",
    "\n",
    "def _make_sparse_csr(data, rows, cols, dtype=DTYPE):\n",
    "    # check lengths\n",
    "    check_consistent_length(data, rows, cols)\n",
    "    data, rows, cols = (np.asarray(x) for x in (data, rows, cols))\n",
    "\n",
    "    shape = (np.unique(rows).shape[0], np.unique(cols).shape[0])\n",
    "    return sparse.csr_matrix((data, (rows, cols)),\n",
    "                             shape=shape, dtype=dtype)\n",
    "\n",
    "\n",
    "def to_sparse_csr(u, i, r, axis=0, dtype=DTYPE):\n",
    "    \"\"\"Create a sparse ratings matrix.\n",
    "    Create a sparse ratings matrix with users and items as rows and columns,\n",
    "    and ratings as the values.\n",
    "    Parameters\n",
    "    ----------\n",
    "    u : array-like, shape=(n_samples,)\n",
    "        The user vector. Positioned along the row axis if ``axis=0``,\n",
    "        otherwise positioned along the column axis.\n",
    "    i : array-like, shape=(n_samples,)\n",
    "        The item vector. Positioned along the column axis if ``axis=0``,\n",
    "        otherwise positioned along the row axis.\n",
    "    r : array-like, shape=(n_samples,)\n",
    "        The ratings vector.\n",
    "    axis : int, optional (default=0)\n",
    "        The axis along which to position the users. If 0, the users are\n",
    "        along the rows (with items as columns). If 1, the users are columns\n",
    "        with items as rows.\n",
    "    dtype : type, optional (default=np.float32)\n",
    "        The type of the values in the ratings matrix.\n",
    "    \"\"\"\n",
    "    if axis not in (0, 1):\n",
    "        raise ValueError(\"axis must be an int in (0, 1)\")\n",
    "\n",
    "    rows = u if axis == 0 else i\n",
    "    cols = i if axis == 0 else u\n",
    "    return _make_sparse_csr(data=r, rows=rows, cols=cols, dtype=dtype)\n",
    "\n",
    "\n",
    "def check_cv(cv=3):\n",
    "    \"\"\"Input validation for cross-validation classes.\n",
    "    Parameters\n",
    "    ----------\n",
    "    cv : int, None or BaseCrossValidator\n",
    "        The CV class or number of folds.\n",
    "        - None will default to 3-fold BootstrapCV\n",
    "        - integer will default to ``integer``-fold BootstrapCV\n",
    "        - BaseCrossValidator will pass through untouched\n",
    "    Returns\n",
    "    -------\n",
    "    checked_cv : BaseCrossValidator\n",
    "        The validated CV class\n",
    "    \"\"\"\n",
    "    if cv is None:\n",
    "        cv = 3\n",
    "\n",
    "    if isinstance(cv, numbers.Integral):\n",
    "        return BootstrapCV(n_splits=int(cv))\n",
    "    if not hasattr(cv, \"split\") or isinstance(cv, six.string_types):\n",
    "        raise ValueError(\"Expected integer or CV class, but got %r (type=%s)\"\n",
    "                         % (cv, type(cv)))\n",
    "    return cv\n",
    "\n",
    "\n",
    "def _validate_train_size(train_size):\n",
    "    \"\"\"Train size should be a float between 0 and 1.\"\"\"\n",
    "    assert isinstance(train_size, float) and (0. < train_size < 1.), \\\n",
    "        \"train_size should be a float between 0 and 1\"\n",
    "\n",
    "\n",
    "def _get_stratified_tr_mask(u, i, train_size, random_state):\n",
    "    _validate_train_size(train_size)  # validate it's a float\n",
    "    random_state = check_random_state(random_state)\n",
    "    n_events = u.shape[0]\n",
    "\n",
    "    # this is our train mask that we'll update over the course of this method\n",
    "    train_mask = random_state.rand(n_events) <= train_size  # type: np.ndarray\n",
    "\n",
    "    # we have a random mask now. For each of users and items, determine which\n",
    "    # are missing from the mask and randomly select one of each of their\n",
    "    # ratings to force them into the mask\n",
    "    for array in (u, i):\n",
    "        # e.g.:\n",
    "        # >>> array = np.array([1, 2, 3, 3, 1, 3, 2])\n",
    "        # >>> train_mask = np.array([0, 1, 1, 1, 0, 0, 1]).astype(bool)\n",
    "        # >>> unique, counts = np.unique(array, return_counts=True)\n",
    "        # >>> unique, counts\n",
    "        # (array([1, 2, 3]), array([2, 2, 3]))\n",
    "\n",
    "        # then present:\n",
    "        # >>> present\n",
    "        # array([2, 3, 3, 2])\n",
    "        present = array[train_mask]\n",
    "\n",
    "        # and the test indices:\n",
    "        # >>> test_vals\n",
    "        # array([1, 1, 3])\n",
    "        test_vals = array[~train_mask]\n",
    "\n",
    "        # get the test indices that are NOT present (either\n",
    "        # missing items or users)\n",
    "        # >>> missing\n",
    "        # array([1])\n",
    "        missing = np.unique(test_vals[np.where(\n",
    "            ~np.in1d(test_vals, present))[0]])\n",
    "\n",
    "        # If there is nothing missing, we got perfectly lucky with our random\n",
    "        # split and we'll just go with it...\n",
    "        if missing.shape[0] == 0:\n",
    "            continue\n",
    "\n",
    "        # Otherwise, if we get to this point, we have to add in the missing\n",
    "        # level to the mask to make sure at least one of each of those makes\n",
    "        # it into the training data (so we don't lose a factor level for ALS)\n",
    "        array_mask_missing = np.in1d(array, missing)\n",
    "\n",
    "        # indices in \"array\" where we have a level that's currently missing\n",
    "        # and that needs to be added into the mask\n",
    "        where_missing = np.where(array_mask_missing)[0]  # e.g., array([0, 4])\n",
    "\n",
    "        # I don't love having to loop here... but we'll iterate \"where_missing\"\n",
    "        # to incrementally add in items or users until all are represented\n",
    "        # in the training set to some degree\n",
    "        added = set()\n",
    "        for idx, val in zip(where_missing, array[where_missing]):\n",
    "            # if we've already seen and added this one\n",
    "            if val in added:  # O(1) lookup\n",
    "                continue\n",
    "\n",
    "            train_mask[idx] = True\n",
    "            added.add(val)\n",
    "\n",
    "    return train_mask\n",
    "\n",
    "\n",
    "def _make_sparse_tr_te(users, items, ratings, train_mask):\n",
    "    # now make the sparse matrices\n",
    "    r_train = to_sparse_csr(u=users[train_mask], i=items[train_mask],\n",
    "                            r=ratings[train_mask], axis=0)\n",
    "\n",
    "    # TODO: anti mask for removing from test set?\n",
    "    r_test = to_sparse_csr(u=users, i=items, r=ratings, axis=0)\n",
    "    return r_train, r_test\n",
    "\n",
    "\n",
    "def train_test_split(u, i, r, train_size=0.75, random_state=None):\n",
    "    \"\"\"Create a train/test split for sparse ratings.\n",
    "    Given vectors of users, items, and ratings, create a train/test split\n",
    "    that preserves at least one of each user and item in the training split\n",
    "    to prevent inducing a cold-start situation.\n",
    "    Parameters\n",
    "    ----------\n",
    "    u : array-like, shape=(n_samples,)\n",
    "        A numpy array of the users. This vector will be used to stratify the\n",
    "        split to ensure that at least of each of the users will be included\n",
    "        in the training split. Note that this diminishes the likelihood of a\n",
    "        perfectly-sized split (i.e., ``len(train)`` may not exactly equal\n",
    "        ``train_size * n_samples``).\n",
    "    i : array-like, shape=(n_samples,)\n",
    "        A numpy array of the items. This vector will be used to stratify the\n",
    "        split to ensure that at least of each of the items will be included\n",
    "        in the training split. Note that this diminishes the likelihood of a\n",
    "        perfectly-sized split (i.e., ``len(train)`` may not exactly equal\n",
    "        ``train_size * n_samples``).\n",
    "    r : array-like, shape=(n_samples,)\n",
    "        A numpy array of the ratings.\n",
    "    train_size : float, optional (default=0.75)\n",
    "        The ratio of the train set size. Should be a float between 0 and 1.\n",
    "    random_state : RandomState, int or None, optional (default=None)\n",
    "        The random state used to create the train mask.\n",
    "    Examples\n",
    "    --------\n",
    "    An example of a sparse matrix split that masks some ratings from the train\n",
    "    set, but not from the testing set:\n",
    "    >>> u = [0, 1, 0, 2, 1, 3]\n",
    "    >>> i = [1, 2, 2, 0, 3, 2]\n",
    "    >>> r = [0.5, 1.0, 0.0, 1.0, 0.0, 1.]\n",
    "    >>> train, test = train_test_split(u, i, r, train_size=0.5,\n",
    "    ...                                random_state=42)\n",
    "    >>> train.toarray()\n",
    "    array([[ 0. ,  0.5,  0. ,  0. ],\n",
    "           [ 0. ,  0. ,  0. ,  0. ],\n",
    "           [ 1. ,  0. ,  0. ,  0. ],\n",
    "           [ 0. ,  0. ,  1. ,  0. ]], dtype=float32)\n",
    "    >>> test.toarray()\n",
    "    array([[ 0. ,  0.5,  0. ,  0. ],\n",
    "           [ 0. ,  0. ,  1. ,  0. ],\n",
    "           [ 1. ,  0. ,  0. ,  0. ],\n",
    "           [ 0. ,  0. ,  1. ,  0. ]], dtype=float32)\n",
    "    Here's a more robust example (with more ratings):\n",
    "    >>> from sklearn.preprocessing import LabelEncoder\n",
    "    >>> import numpy as np\n",
    "    >>> rs = np.random.RandomState(42)\n",
    "    >>> users = np.arange(100000)  # 100k users in DB\n",
    "    >>> items = np.arange(30000)  # 30k items in DB\n",
    "    >>> # Randomly select some for ratings:\n",
    "    >>> items = rs.choice(items, users.shape[0])  # 100k rand item rtgs\n",
    "    >>> users = rs.choice(users, users.shape[0])  # 100k rand user rtgs\n",
    "    >>> # Label encode so they're positional indices:\n",
    "    >>> users = LabelEncoder().fit_transform(users)\n",
    "    >>> items = LabelEncoder().fit_transform(items)\n",
    "    >>> ratings = rs.choice((0., 0.25, 0.5, 0.75, 1.), items.shape[0])\n",
    "    >>> train, test = train_test_split(users, items, ratings, random_state=rs)\n",
    "    >>> train\n",
    "    <26353x28921 sparse matrix of type '<type 'numpy.float32'>'\n",
    "        with 77770 stored elements in Compressed Sparse Row format>\n",
    "    >>> test\n",
    "    <26353x28921 sparse matrix of type '<type 'numpy.float32'>'\n",
    "        with 99994 stored elements in Compressed Sparse Row format>\n",
    "        \n",
    "    Notes\n",
    "    -----\n",
    "    ``u``, ``i`` inputs should be encoded (i.e., via LabelEncoder) prior to\n",
    "    splitting the data. This is due to the indexing behavior used within the\n",
    "    function.\n",
    "    Returns\n",
    "    -------\n",
    "    r_train : scipy.sparse.csr_matrix\n",
    "        The train set.\n",
    "    r_test : scipy.sparse.csr_matrix\n",
    "        The test set.\n",
    "    \"\"\"\n",
    "    # make sure all of them are numpy arrays and of the same length\n",
    "    users, items, ratings = check_consistent_length(u, i, r)\n",
    "\n",
    "    train_mask = _get_stratified_tr_mask(\n",
    "        users, items, train_size=train_size,\n",
    "        random_state=random_state)\n",
    "\n",
    "    return _make_sparse_tr_te(users, items, ratings, train_mask=train_mask)\n",
    "\n",
    "\n",
    "# avoid pb w nose\n",
    "train_test_split.__test__ = False\n",
    "\n",
    "\n",
    "class BaseCrossValidator(six.with_metaclass(ABCMeta)):\n",
    "    \"\"\"Base class for all collab CV.\n",
    "    Iterations must define ``_iter_train_mask``. This is based loosely\n",
    "    on sklearn's cross validator but does not adhere to its exact\n",
    "    interface.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_splits=3, random_state=None):\n",
    "        self.n_splits = n_splits\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def get_n_splits(self):\n",
    "        return self.n_splits\n",
    "\n",
    "    def split(self, X):\n",
    "        \"\"\"Generate indices to split data into training and test sets.\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : scipy.sparse.csr_matrix\n",
    "            A sparse ratings matrix.\n",
    "        Returns\n",
    "        -------\n",
    "        train : scipy.sparse.csr_matrix\n",
    "            The training set\n",
    "        test : scipy.sparse.csr_matrix\n",
    "            The test set\n",
    "        \"\"\"\n",
    "        ratings = X.data\n",
    "        users, items = X.nonzero()\n",
    "\n",
    "        # make sure all of them are numpy arrays and of the same length\n",
    "        # users, items, ratings = check_consistent_length(u, i, r)\n",
    "        for train_mask in self._iter_train_mask(users, items, ratings):\n",
    "            # yield in a generator so we don't have to store in mem\n",
    "            yield _make_sparse_tr_te(users, items, ratings,\n",
    "                                     train_mask=train_mask)\n",
    "\n",
    "    @abstractmethod\n",
    "    def _iter_train_mask(self, u, i, r):\n",
    "        \"\"\"Compute the training mask here.\n",
    "        Returns\n",
    "        -------\n",
    "        train_mask : np.ndarray\n",
    "            The train mask\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "class BootstrapCV(BaseCrossValidator):\n",
    "    \"\"\"Cross-validate with bootstrapping.\n",
    "    The bootstrap CV class makes no guarantees about exclusivity between folds.\n",
    "    This is simply a naive way to handle KFold cross-validation for something as\n",
    "    complex as a collaborative filtering split.\n",
    "    \"\"\"\n",
    "\n",
    "    def _iter_train_mask(self, u, i, r):\n",
    "        \"\"\"Compute the training mask here.\"\"\"\n",
    "        train_size = 1. - (1. / self.n_splits)\n",
    "        # train_size = 1. - ((n_samples / self.n_splits) / n_samples)\n",
    "        random_state = check_random_state(self.random_state)\n",
    "\n",
    "        for split in range(self.n_splits):\n",
    "            yield _get_stratified_tr_mask(\n",
    "                u, i, train_size=train_size,\n",
    "                random_state=random_state.randint(MAX_SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "czech-traffic",
   "metadata": {},
   "outputs": [],
   "source": [
    "rs = np.random.RandomState(42)\n",
    "users = LabelEncoder().fit_transform(ratings['user'])\n",
    "items = LabelEncoder().fit_transform(ratings['item'])\n",
    "train_data, test_data = train_test_split(users, items, ratings['rating'], random_state=rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "viral-entrepreneur",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = BootstrapCV(n_splits=3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "australian-server",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "latent10\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "latent20\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "latent30\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "latent40\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "latent50\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n",
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "steps=300\n",
    "alpha=0.0002\n",
    "beta=0.02\n",
    "score_dict = {\"latent\":[],\"score\":[]};\n",
    "for K in range(10,51,10):\n",
    "    print('latent' + str(K))\n",
    "    score = 0\n",
    "    for j in cv.split(train_data): \n",
    "        train, test = j # watch out for K, it shouldn't exceed size of train\n",
    "        P = np.random.rand(train.shape[0],K)\n",
    "        Q = np.random.rand(train.shape[1],K)\n",
    "        Mask_tr = (train!=0)\n",
    "        Mask_te = (test!=0)\n",
    "        matrix_estimated,_,_,_ = matrix_factorization_pred(train.todense(),P,Q,K,steps,alpha,beta,Mask_tr.todense())\n",
    "        extimated_error = np.multiply(test.todense() - matrix_estimated, Mask_te.todense())\n",
    "        score = score + LA.norm(extimated_error)\n",
    "    score = score/cv.n_splits\n",
    "    score_dict[\"latent\"].append(K)\n",
    "    score_dict[\"score\"].append(score)                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "honey-privilege",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'latent': [10, 20, 30, 40, 50],\n",
       " 'score': [87.51198472912883,\n",
       "  82.29717141934985,\n",
       "  82.66962947347582,\n",
       "  153.47746847982228,\n",
       "  153.47638254793472]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
