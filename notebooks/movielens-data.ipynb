{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MovieLens Exploratory Dataset\n",
    "**Purpose:** \n",
    "\n",
    "This notebook explores the distribution of input movielens datasets and preprocesses it for the RecSys\n",
    "\n",
    "**Methodology:**\n",
    "\n",
    "The notebook assumes input from the [MovieLens Dataset](https://grouplens.org/datasets/movielens/). It will explore the dataset using basic statistics and explores properties of hidden clusters. It outputs a processed dataset of the following format: \n",
    "\n",
    "**Author:**\n",
    "\n",
    "Prajna Soni (@prajnasoni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Import the libraries we will be using\n",
    "\n",
    "#import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "#import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import logging\n",
    "\n",
    "from sklearn import metrics\n",
    "#from sklearn.tree import DecisionTreeClassifier\n",
    "#from sklearn.model_selection import train_test_split\n",
    "#from IPython.display import Image\n",
    "#from sklearn.tree import export_graphviz\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "#import plotly\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MovieLens dataset\n",
    "\n",
    "# from lenskit.datasets import ML100K\n",
    "# movielens = ML100K('ml-100k')\n",
    "# ratings = movielens.ratings\n",
    "# movies = movielens.movies\n",
    "\n",
    "movies = pd.read_csv(\"../datasets/movielens-small/movies.csv\")\n",
    "ratings = pd.read_csv(\"../datasets/movielens-small/ratings.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Timestamp is the timestamp since January 1, 1970 in seconds\n",
    "# Reduce dimensionality of time so it starts from earliest time as 0 seconds\n",
    "min_t = ratings['timestamp'].min()\n",
    "ratings['timestamp'] = ratings['timestamp']-min_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOVIELENS DATASET CLASS - easy access functions to process movielens dataset\n",
    "class movielens:\n",
    "    \n",
    "     # constructor taking in dataset (genre_ratings), number of maximum clusters\n",
    "    def __init__(self, movies, ratings):\n",
    "        \n",
    "        # assign input rating matrix\n",
    "        self.movies = movies\n",
    "        self.ratings = ratings\n",
    "        \n",
    "        # Identify genres in dataset\n",
    "        self.get_dummy_genres()\n",
    "        \n",
    "        # Enable logging\n",
    "        self._logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'MovieLens Dataset'\n",
    "    \n",
    "    # Function to return list of strings of genres in MovieLens dataset\n",
    "    def get_genres(self):\n",
    "        return self.genres\n",
    "        \n",
    "    # Function to return dataframe of user (rows) and movie (columns) ratings - a user-item interaction matrix\n",
    "    def UserItem(self):\n",
    "        self.UI_matrix = self.ratings.merge(movies,on='movieId', how='left')\n",
    "        self.UI_matrix = self.UI_matrix.pivot_table(index='userId',columns='title',values='rating')\n",
    "        self.UI_matrix = self.UI_matrix.fillna(0)\n",
    "        return self.UI_matrix \n",
    "    \n",
    "    # Function to get the genre ratings\n",
    "    def UserGenreRatings(self):\n",
    "        self.genre_ratings = pd.DataFrame()\n",
    "        for genre in self.get_genres():        \n",
    "            genre_movies = self.movies[self.movies['genres'].str.contains(genre)]\n",
    "            avg_genre_votes_per_user = self.ratings[self.ratings['movieId'].isin(genre_movies['movieId'])].loc[:, ['userId', 'rating']].groupby(['userId'])['rating'].mean().round(2)\n",
    "            self.genre_ratings = pd.concat([self.genre_ratings, avg_genre_votes_per_user], axis=1)    \n",
    "        self.genre_ratings = self.genre_ratings.fillna(0)\n",
    "        self.genre_ratings.columns = self.get_genres()\n",
    "        return self.genre_ratings\n",
    "    \n",
    "    # Function to get Weighted genre ratings for each user\n",
    "    # weighted by number of genres a user has rated divided by total number of movies rated\n",
    "    def w_UserGenreRatings(self): \n",
    "        w1 = pd.DataFrame()\n",
    "        for genre in self.get_genres():\n",
    "            temp = self.UserGenreCounts()[genre].div(self.TotalUserRatings()['total_ratings'])\n",
    "            w1[genre] = temp\n",
    "        self.wGR_matrix = dataGR.mul(w1)\n",
    "        return self.wGR_matrix\n",
    "\n",
    "    # Function to get the number of ratings per genre per user\n",
    "    def UserGenreCounts(self):\n",
    "        self.genre_counts = pd.DataFrame()\n",
    "        for genre in self.get_genres():        \n",
    "            genre_movies = self.movies[self.movies['genres'].str.contains(genre) ]\n",
    "            genre_counts_per_user = self.ratings[self.ratings['movieId'].isin(genre_movies['movieId'])].loc[:, ['userId', 'rating']].groupby(['userId'])['rating'].count()\n",
    "            self.genre_counts = pd.concat([self.genre_counts, genre_counts_per_user], axis=1).fillna(0)   \n",
    "        self.genre_counts.columns = self.genres\n",
    "        return self.genre_counts\n",
    "\n",
    "    # Function to count total number of movies a user has rated\n",
    "    def TotalUserRatings(self):\n",
    "        total_user_ratings = self.ratings.groupby(['userId']).count().drop(columns = ['movieId','timestamp'], axis = 1)\n",
    "        total_user_ratings.columns = ['total_ratings']\n",
    "        return total_user_ratings\n",
    "\n",
    "    # Function to split movie genres into dummy variables\n",
    "    def get_dummy_genres(self):\n",
    "        genres_list = self.movies['genres'].str.split(pat='|') # convert string to list of string\n",
    "        self.movies2 = pd.concat([self.movies.drop(['genres','title'],axis=1), genres_list.str.join('|').str.get_dummies()], axis=1) # concatenate dummy variables df of genres\n",
    "        self.genres = self.movies2.columns.tolist()[1:]\n",
    "        return self.movies2\n",
    "    \n",
    "    def SVDmatrix(self, n, dataset='UI'):\n",
    "        if dataset == 'UI':\n",
    "            self.UserItem()\n",
    "            self.UI_SVD =  TruncatedSVD(n_components = n)\n",
    "            self.UI = pd.DataFrame(self.UI_SVD.fit_transform(self.UI_matrix))\n",
    "            self.UI.index += 1\n",
    "            return self.UI\n",
    "        elif dataset == 'GR':\n",
    "            self.UserGenreRatings()\n",
    "            self.GR_SVD =  TruncatedSVD(n_components = n)\n",
    "            self.GR = pd.DataFrame(self.GR_SVD.fit_transform(self.genre_ratings))\n",
    "            self.GR.index += 1\n",
    "            return self.GR\n",
    "        elif dataset == 'wGR':\n",
    "            self.w_UserGenreRatings()\n",
    "            self.wGR_SVD =  TruncatedSVD(n_components = n)\n",
    "            self.wGR = pd.DataFrame(self.wGR_SVD.fit_transform(self.wGR_matrix))\n",
    "            self.wGR.index += 1\n",
    "            return self.wGR\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MOVIELENS DATASET CLASS - easy access functions to process movielens dataset\n",
    "class movielens:\n",
    "    \n",
    "     # constructor taking in dataset (genre_ratings), number of maximum clusters\n",
    "    def __init__(self, movies, ratings):\n",
    "        \n",
    "        # assign input rating matrix\n",
    "        self.movies = movies\n",
    "        self.ratings = ratings\n",
    "        \n",
    "        # Identify genres in dataset\n",
    "        self.get_dummy_genres()\n",
    "        \n",
    "        # Enable logging\n",
    "        self._logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'MovieLens Dataset'\n",
    "    \n",
    "    # Function to return list of strings of genres in MovieLens dataset\n",
    "    def get_genres(self):\n",
    "        return self.genres\n",
    "        \n",
    "    # Function to return dataframe of user (rows) and movie (columns) ratings - a user-item interaction matrix\n",
    "    def UserItem(self):\n",
    "        self.UI_matrix = self.ratings.merge(movies,on='movieId', how='left')\n",
    "        self.UI_matrix = self.UI_matrix.pivot_table(index='userId',columns='title',values='rating')\n",
    "        self.UI_matrix = self.UI_matrix.fillna(0)\n",
    "        return self.UI_matrix \n",
    "    \n",
    "    # Function to get the genre ratings\n",
    "    def UserGenreRatings(self):\n",
    "        self.genre_ratings = pd.DataFrame()\n",
    "        for genre in self.get_genres():        \n",
    "            genre_movies = self.movies[self.movies['genres'].str.contains(genre)]\n",
    "            avg_genre_votes_per_user = self.ratings[self.ratings['movieId'].isin(genre_movies['movieId'])].loc[:, ['userId', 'rating']].groupby(['userId'])['rating'].mean().round(2)\n",
    "            self.genre_ratings = pd.concat([self.genre_ratings, avg_genre_votes_per_user], axis=1)    \n",
    "        self.genre_ratings = self.genre_ratings.fillna(0)\n",
    "        self.genre_ratings.columns = self.get_genres()\n",
    "        return self.genre_ratings\n",
    "    \n",
    "    # Function to get Weighted genre ratings for each user\n",
    "    # weighted by number of genres a user has rated divided by total number of movies rated\n",
    "    def w_UserGenreRatings(self): \n",
    "        w1 = pd.DataFrame()\n",
    "        for genre in self.get_genres():\n",
    "            temp = self.UserGenreCounts()[genre].div(self.TotalUserRatings()['total_ratings'])\n",
    "            w1[genre] = temp\n",
    "        self.wGR_matrix = dataGR.mul(w1)\n",
    "        return self.wGR_matrix\n",
    "\n",
    "    # Function to get the number of ratings per genre per user\n",
    "    def UserGenreCounts(self):\n",
    "        self.genre_counts = pd.DataFrame()\n",
    "        for genre in self.get_genres():        \n",
    "            genre_movies = self.movies[self.movies['genres'].str.contains(genre) ]\n",
    "            genre_counts_per_user = self.ratings[self.ratings['movieId'].isin(genre_movies['movieId'])].loc[:, ['userId', 'rating']].groupby(['userId'])['rating'].count()\n",
    "            self.genre_counts = pd.concat([self.genre_counts, genre_counts_per_user], axis=1).fillna(0)   \n",
    "        self.genre_counts.columns = self.genres\n",
    "        return self.genre_counts\n",
    "\n",
    "    # Function to count total number of movies a user has rated\n",
    "    def TotalUserRatings(self):\n",
    "        total_user_ratings = self.ratings.groupby(['userId']).count().drop(columns = ['movieId','timestamp'], axis = 1)\n",
    "        total_user_ratings.columns = ['total_ratings']\n",
    "        return total_user_ratings\n",
    "\n",
    "    # Function to split movie genres into dummy variables\n",
    "    def get_dummy_genres(self):\n",
    "        genres_list = self.movies['genres'].str.split(pat='|') # convert string to list of string\n",
    "        self.movies2 = pd.concat([movies.drop(['genres','title'],axis=1), genres_list.str.join('|').str.get_dummies()], axis=1) # concatenate dummy variables df of genres\n",
    "        self.genres = self.movies2.columns.tolist()[1:]\n",
    "        return self.movies2\n",
    "    \n",
    "    def SVDmatrix(self, n, dataset='UI'):\n",
    "        if dataset == 'UI':\n",
    "            self.UserItem()\n",
    "            self.UI_SVD =  TruncatedSVD(n_components = n)\n",
    "            self.UI = pd.DataFrame(self.UI_SVD.fit_transform(self.UI_matrix))\n",
    "            self.UI.index += 1\n",
    "            return self.UI\n",
    "        elif dataset == 'GR':\n",
    "            self.UserGenreRatings()\n",
    "            self.GR_SVD =  TruncatedSVD(n_components = n)\n",
    "            self.GR = pd.DataFrame(self.GR_SVD.fit_transform(self.genre_ratings))\n",
    "            self.GR.index += 1\n",
    "            return self.GR\n",
    "        elif dataset == 'wGR':\n",
    "            self.w_UserGenreRatings()\n",
    "            self.wGR_SVD =  TruncatedSVD(n_components = n)\n",
    "            self.wGR = pd.DataFrame(self.wGR_SVD.fit_transform(self.wGR_matrix))\n",
    "            self.wGR.index += 1\n",
    "            return self.wGR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MovieLens Object with DFs from loaded CSVs\n",
    "data = movielens(movies, ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.SVDmatrix(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate total ratings of each unique rating value (in 0.5 increments)\n",
    "rating_count = ratings['rating'].value_counts().sort_values(ascending=False)\n",
    "# Calculate each user's mean ratings across all movies rated by that user \n",
    "avg_user = ratings.drop(columns = ['movieId','timestamp'], axis = 1).groupby(['userId']).mean()\n",
    "# Calculate \n",
    "rating_range = [0,1,2,3,4,5]\n",
    "avg_movie = ratings.drop(columns = ['userId','timestamp'], axis = 1).groupby(['movieId']).mean()\n",
    "avg_movie = pd.cut(avg_movie['rating'], bins=rating_range, include_lowest=True).value_counts(sort=False)\n",
    "\n",
    "fig, plts = plt.subplots(1,2)\n",
    "fig.set_figwidth(14)\n",
    "fig.set_figheight(5)\n",
    "fig.suptitle('Distribution of Dataset')\n",
    "plts[0].set_title('Distribution of Total Ratings (Count)',fontsize=12)\n",
    "plts[0].bar(x=rating_count.index,height=rating_count.values, width=0.3)\n",
    "\n",
    "plts[1].set_title('Mean Ratings per User = ' + str(avg_user.mean().values)+'/5.00',fontsize=12)\n",
    "plts[1].plot(avg_user.index,avg_user.values)\n",
    "plts[1].set_ylim(0,5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_movie.plot.bar(title='Distribution of Mean Movie Ratings (Count)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_genres = data.get_dummy_genres()\n",
    "genre_counts = []\n",
    "for genre in data.get_genres():\n",
    "    genre_counts.append(dummy_genres[genre].sum())\n",
    "\n",
    "plt.figure(figsize = (21,5))\n",
    "plt.title('Distribution of movie items across genres', fontsize=15)\n",
    "plt.bar(data.get_genres(),genre_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creation of different datasets for SVD (weighted, absolute)\n",
    "\n",
    "# User-Item Interaction Matrix\n",
    "dataUI = data.UserItem()\n",
    "\n",
    "# Average rating for each genre for each user\n",
    "dataGR = data.UserGenreRatings()\n",
    "\n",
    "# Weighted genre ratings for each user\n",
    "dataGR_w1 = data.w_UserGenreRatings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataGR_w1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sum of weights across a user is greater than 1 as each movie can have more than one genre.\n",
    "The weight in w1[i,j] represents the ratio of movies rated by user[i] in genre[j] to the total number of movies rated by user[i]\n",
    "\n",
    "$$ \\sum_{i=1}^{n} w1[i,j] \\geq 1 \\forall j $$\n",
    "\n",
    "$$ w1[i,j] = \\frac{UserGenreCounts[i,j]}{TotalUserRatings[i]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"sum of weights for each user:\") \n",
    "w1.sum(axis=1).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Correlation of unweighted ratings for sample genres for each user')\n",
    "plt.figure(figsize=(21,7))\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(x=dataGR['Action'].to_numpy(),y=dataGR['Adventure'].to_numpy())\n",
    "plt.xlabel('average action rating')\n",
    "plt.ylabel('average adventure rating')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(x=dataGR['Sci-Fi'].to_numpy(),y=dataGR['Crime'].to_numpy())\n",
    "plt.xlabel('average scifi rating')\n",
    "plt.ylabel('average crime rating')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(x=dataGR['Thriller'].to_numpy(),y=dataGR['Horror'].to_numpy())\n",
    "plt.xlabel('average thriller rating')\n",
    "plt.ylabel('average horror rating')\n",
    "plt.show()\n",
    "\n",
    "print('Correlation of weighted ratings for sample genres for each user')\n",
    "plt.figure(figsize=(21,7))\n",
    "plt.subplot(1,3,1)\n",
    "plt.scatter(x=dataGR_w1['Action'].to_numpy(),y=dataGR_w1['Adventure'].to_numpy())\n",
    "plt.xlabel('average action rating')\n",
    "plt.ylabel('average adventure rating')\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.scatter(x=dataGR_w1['Sci-Fi'].to_numpy(),y=dataGR_w1['Crime'].to_numpy())\n",
    "plt.xlabel('average scifi rating')\n",
    "plt.ylabel('average crime rating')\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.scatter(x=dataGR_w1['Thriller'].to_numpy(),y=dataGR_w1['Horror'].to_numpy())\n",
    "plt.xlabel('average thriller rating')\n",
    "plt.ylabel('average horror rating')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Reduction using SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User-Movie Rating matrix (complete unfiltered rating matrix from MovieLens dataset)\n",
    "dataUI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# Decomposition of Singular Values from SVD for Dimensionality Reduction of UserItem matrix\n",
    "#UI_SVD_full =  TruncatedSVD(n_components = min((len(dataUI)-1),(len(dataUI.columns)-1)))\n",
    "UI_SVD_full =  TruncatedSVD(n_components = 10)\n",
    "UI = pd.DataFrame(UI_SVD_full.fit_transform(dataUI))\n",
    "UI.index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"First 10 singular values from SVD of UserItem Interaction Matrix\")\n",
    "print(UI_SVD_full.singular_values_[:10])\n",
    "\n",
    "# Plot Singular Value Decomposition \n",
    "plt.figure(figsize=(21,5))\n",
    "plt.plot(UI_SVD_full.singular_values_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomposition of Singular Values from SVD for Dimensionality Reduction of UserItem matrix\n",
    "GR_SVD_full =  TruncatedSVD(n_components = len(dataGR.columns)-1)\n",
    "GR = pd.DataFrame(GR_SVD_full.fit_transform(dataGR))\n",
    "GR.index += 1\n",
    "\n",
    "print(\"First 10 singular values from SVD of Unweighted User Genre Rating Matrix\")\n",
    "print(GR_SVD_full.singular_values_[:10])\n",
    "\n",
    "# Plot Singular Value Decomposition \n",
    "plt.figure(figsize=(21,5))\n",
    "plt.plot(GR_SVD_full.singular_values_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decomposition of Singular Values from SVD for Dimensionality Reduction of UserItem matrix\n",
    "GRw1_SVD_full =  TruncatedSVD(n_components = len(dataGR_w1.columns)-1)\n",
    "GRw1 = pd.DataFrame(GRw1_SVD_full.fit_transform(dataGR_w1))\n",
    "GRw1.index += 1\n",
    "\n",
    "print(\"First 10 singular values from SVD of Weighted User Genre Rating Matrix\")\n",
    "print(GRw1_SVD_full.singular_values_[:10])\n",
    "\n",
    "# Plot Singular Value Decomposition \n",
    "plt.figure(figsize=(21,5))\n",
    "plt.plot(GRw1_SVD_full.singular_values_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#UI_temp = UI.iloc[:,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UserInteraction Matrix\n",
    "plt.figure(figsize=(14,5))\n",
    "plt.subplot(1,3,1)\n",
    "plt.title('User Interaction Matrix')\n",
    "plt.scatter(UI[0], UI[1])\n",
    "plt.xlabel('Latent Feature 0')\n",
    "plt.ylabel('Latent Feature 1')\n",
    "\n",
    "# UserGenreRatings Matrix\n",
    "plt.subplot(1,3,2)\n",
    "plt.title('User Genre Rating Matrix')\n",
    "plt.scatter(GR[0], GR[1])\n",
    "plt.xlabel('Latent Feature 0')\n",
    "plt.ylabel('Latent Feature 1')\n",
    "\n",
    "# WeightedUserGenreRatings Matrix\n",
    "plt.subplot(1,3,3)\n",
    "plt.title('Weighted User Genre Rating Matrix')\n",
    "plt.scatter(GRw1[0], GRw1[1])\n",
    "plt.xlabel('Latent Feature 0')\n",
    "plt.ylabel('Latent Feature 1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CLASS TO CLUSTER AND EVALUATE DATA\n",
    "class data:\n",
    "\n",
    "    # constructor taking in dataset (genre_ratings), number of maximum clusters\n",
    "    def __init__(self, data):\n",
    "        # assign input rating matrix\n",
    "        self.data = data \n",
    "        # Enable logging\n",
    "        self._logger = logging.getLogger(__name__)\n",
    "        \n",
    "    def __str__(self):\n",
    "        return 'Data Object'\n",
    "    \n",
    "    # perform kmeans clustering for n clusters on data and return a dataframe with user and cluster number \n",
    "    def kmeans(self, n):\n",
    "        \n",
    "        if n is None:\n",
    "            self._logger.warning('Number of clusters not provided')\n",
    "            return None\n",
    "        \n",
    "        km = KMeans(n_clusters=n, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "        self.km_pred = km.fit_predict(self.data)\n",
    "        self.km_pred = pd.DataFrame(self.km_pred, columns = ['cluster'])\n",
    "        self.km_pred.index += 1 # adjust index to match userId\n",
    "        #clustered_data = pd.concat([self.data, km_pred], axis=1)\n",
    "        return self.km_pred\n",
    "    \n",
    "    # print graphs to evaluate kmeans clustering from 2 to n clusters using kmeans score, silhouette score and davies-bouldin score\n",
    "    def kmeans_eval(self, n):\n",
    "        \n",
    "        if n is None:\n",
    "            self._logger.warning('Number of maximum clusters not provided')\n",
    "            return None\n",
    "        \n",
    "        # variable scope limited to function\n",
    "        km_scores= []\n",
    "        km_silhouette = []\n",
    "        db_score = []\n",
    "        \n",
    "        # calculate scores \n",
    "        for i in range(2,n+1):\n",
    "            km = KMeans(n_clusters=i, random_state=0, max_iter=300).fit(self.data)\n",
    "            km_pred = km.predict(self.data)\n",
    "\n",
    "            #KM Score\n",
    "            km_scores.append(-kmeans.score(self.data))\n",
    "\n",
    "            #Silhouette Score\n",
    "            km_silhouette.append(metrics.silhouette_score(self.data, km_pred))\n",
    "\n",
    "            #Davies Bouldin Score\n",
    "            # the average similarity measure of each cluster with its most similar cluster, \n",
    "            # where similarity is the ratio of within-cluster distances to between-cluster distances. \n",
    "            # Thus, clusters which are farther apart and less dispersed will result in a better score.\n",
    "            db_score.append(metrics.davies_bouldin_score(self.data, km_pred))\n",
    "\n",
    "        # plot graphs of evaluation metrics\n",
    "        # ELBOW METHOD (optimal cluster at elbow in curve)\n",
    "        plt.figure(figsize=(14,21))\n",
    "        plt.subplot(3,1,1)\n",
    "        plt.title(\"The elbow method for determining number of clusters\",fontsize=16)\n",
    "        plt.scatter(x=[i for i in range(2,n+1)],y=km_scores,s=150,edgecolor='k')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(\"Number of clusters\",fontsize=14)\n",
    "        plt.ylabel(\"K-means Score\",fontsize=15)\n",
    "        plt.xticks([i for i in range(2,n+1)],fontsize=14)\n",
    "        plt.yticks(fontsize=15)\n",
    "        \n",
    "        # SILHOUETTE SCORE (silhouette score varies from [-1,1] with 1 meaning clearly defined clusters)\n",
    "        plt.subplot(3,1,2)\n",
    "        plt.title(\"The silhouette coefficient method for determining number of clusters (1 is ideal)\",fontsize=16)\n",
    "        plt.scatter(x=[i for i in range(2,n+1)],y=km_silhouette,s=150,edgecolor='k')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(\"Number of clusters\",fontsize=14)\n",
    "        plt.ylabel(\"Average Silhouette Score\",fontsize=15)\n",
    "        plt.ylim(-1,1)\n",
    "        plt.xticks([i for i in range(2,n+1)],fontsize=14)\n",
    "        plt.yticks(fontsize=15)\n",
    "       \n",
    "        # DAVIES-BOULDIN SCORE (lower score is better and means more disctinct clusters)\n",
    "        plt.subplot(3,1,3)\n",
    "        plt.title(\"The davies-bouldin coefficient method for determining number of clusters (0 is ideal)\",fontsize=16)\n",
    "        plt.scatter(x=[i for i in range(2,n+1)],y=db_score,s=150,edgecolor='k')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(\"Number of clusters\")\n",
    "        plt.ylabel(\"Davies-Bouldin Score\")\n",
    "        plt.ylim(bottom = 0)\n",
    "        plt.xticks([i for i in range(2,n+1)],fontsize=14)\n",
    "        plt.yticks(fontsize=15)\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    # perform GaussianMixture clustering for n clusters on data and return a dataframe with user and cluster number\n",
    "    def gmm(self, n, covariance_type='full', df='None'):\n",
    "        # n = number of clusters\n",
    "        # covariance_type is 'full', 'diag', 'tied' or 'spherical'\n",
    "        # df is 'pred' for cluster predictions, 'proba' for cluster probabilities, and 'full' for input data combined with probabilities\n",
    "        if n is None:\n",
    "            self._logger.warning('Number of maximum clusters not provided')\n",
    "            return None\n",
    "        \n",
    "        if covariance_type is None:\n",
    "            self._logger.warning('Covariance Type for Gaussian Mixture Model not provided. Default is \"full\".')\n",
    "            return None\n",
    "        \n",
    "        if df is None:\n",
    "            self._logger.warning('Return df format not provided. Default is \"pred\".')\n",
    "            return None\n",
    "        \n",
    "        gmm = GaussianMixture(n_components=n, n_init=10, covariance_type=covariance_type, tol=1e-3, max_iter=500)\n",
    "        self.gmm_pred = gmm.fit_predict(self.data)\n",
    "        self.gmm_pred = pd.DataFrame(self.gmm_pred, columns = ['cluster'])\n",
    "        \n",
    "        # Return new datafram with clusters, and probability of belonging to a cluster \n",
    "        if df == 'pred':\n",
    "            self.gmm_pred.index += 1\n",
    "            return self.gmm_pred\n",
    "        elif df == 'proba':\n",
    "            cols = ['proba_C'+str(int) for int in range(n)]\n",
    "            proba = self.gmm_pred.join(pd.DataFrame(gmm.predict_proba(UI_temp), columns = cols))\n",
    "            proba.index += 1 # adjust index to match userId\n",
    "            return proba\n",
    "        elif df == 'all':\n",
    "            cols = ['proba_C'+str(int) for int in range(n)]\n",
    "            proba = self.gmm_pred.join(pd.DataFrame(gmm.predict_proba(UI_temp), columns = cols))\n",
    "            proba.index += 1 # adjust index to match userId\n",
    "            full = self.data.join(proba ,how='left')\n",
    "            return full\n",
    "        elif df == 'None':\n",
    "            return None\n",
    "        else:\n",
    "            self._logger.error(\"Invalid input. Enter 'all', 'pred' or 'proba'.\")\n",
    "            return None\n",
    "    \n",
    "    # print graphs to evaluate kmeans clustering from 2 to n clusters using \n",
    "    def gmm_eval(self, n, covariance_type=\"full\"):\n",
    "        \n",
    "        if n is None:\n",
    "            self._logger.error('Number of maximum clusters not provided')\n",
    "            return None\n",
    "        \n",
    "        if covariance_type is None:\n",
    "            self._logger.warning('Covariance Type for Gaussian Mixture Model not provided. Default is \"full\"')\n",
    "            return None\n",
    "        \n",
    "        # variable scope limited to function\n",
    "        gmm_aic = []\n",
    "        gmm_bic = []\n",
    "        gmm_scores = [] \n",
    "        \n",
    "        # calculate scores \n",
    "        for i in range(2,n+1):\n",
    "            gmm = GaussianMixture(n_components=i,n_init=10, covariance_type = covariance_type, tol=1e-3,max_iter=500).fit(self.data)\n",
    "            \n",
    "            # Akaike Information Criterion\n",
    "            gmm_aic.append(gmm.aic(self.data))\n",
    "            \n",
    "            # Bayesian Information Criterion\n",
    "            gmm_bic.append(gmm.bic(self.data))\n",
    "            \n",
    "            gmm_scores.append(gmm.score(self.data))\n",
    "            \n",
    "        # Plot the scores \n",
    "        plt.figure(figsize=(14,21))\n",
    "        plt.subplot(3,1,1)\n",
    "        #plt.title(\"The Gaussian Mixture model AIC for determining number of clusters, CT = \"+covariance_type,fontsize=16)\n",
    "        plt.scatter(x=[i for i in range(2,n+1)],y=np.log(gmm_aic),s=150,edgecolor='k')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(\"Number of clusters\",fontsize=14)\n",
    "        plt.ylabel(\"Log of Gaussian mixture AIC score\",fontsize=15)\n",
    "        plt.xticks([i for i in range(2,n+1)],fontsize=14)\n",
    "        plt.yticks(fontsize=15)\n",
    "\n",
    "        plt.subplot(3,1,2)\n",
    "        #plt.title(\"The Gaussian Mixture model BIC for determining number of clusters, CT = \"+covariance_type,fontsize=16)\n",
    "        plt.scatter(x=[i for i in range(2,n+1)],y=np.log(gmm_bic),s=150,edgecolor='k')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(\"Number of clusters\",fontsize=14)\n",
    "        plt.ylabel(\"Log of Gaussian mixture BIC score\",fontsize=15)\n",
    "        plt.xticks([i for i in range(2,n+1)],fontsize=14)\n",
    "        plt.yticks(fontsize=15)\n",
    "   \n",
    "        plt.subplot(3,1,3)\n",
    "        #plt.title(\"The Gaussian Mixture model scores for determining number of clusters, CT = \"+covariance_type,fontsize=16)\n",
    "        plt.scatter(x=[i for i in range(2,n+1)],y=gmm_scores,s=150,edgecolor='k')\n",
    "        plt.grid(True)\n",
    "        plt.xlabel(\"Number of clusters\",fontsize=14)\n",
    "        plt.ylabel(\"Gaussian mixture score\",fontsize=15)\n",
    "        plt.xticks([i for i in range(2,n+1)],fontsize=14)\n",
    "        plt.yticks(fontsize=15)\n",
    "        plt.show()\n",
    "\n",
    "    def plotScatter(self, show_cluster, model):\n",
    "        \n",
    "        # logger warning if no clusters to plot/colour  \n",
    "        if show_cluster:\n",
    "            if model == 'gmm':\n",
    "                if self.gmm_pred is None:\n",
    "                    self._logger.error(\"Gaussian Mixture Model not trained. Use data.gmm(n, covariance_type, df) to train before plotting\")\n",
    "                    return None\n",
    "                clusters = self.gmm_pred\n",
    "            elif model == 'kmeans':\n",
    "                if self.km_pred is None:\n",
    "                    self._logger.error(\"K-Means Model not trained. Use data.kmeans(n) to train before plotting\")\n",
    "                    return None\n",
    "                clusters = self.km_pred\n",
    "            marker = {'size': 3,'opacity': 0.8,'color':clusters['cluster'],'colorscale':'Viridis'}\n",
    "        else:\n",
    "            marker = {'size': 3,'opacity': 0.8,'colorscale':'Viridis'}\n",
    "        \n",
    "        # check input dataset to plot\n",
    "        if len(self.data.columns) >= 3:\n",
    "            if len(self.data.columns) > 3:\n",
    "                self._logger.warning(\"Input dataset contains more than 3 features. 3D scatter plot will only plot first 3 features.\")            \n",
    "            \n",
    "            # plot 3D scatter plot\n",
    "            # Configure Plotly to be rendered inline in the notebook.\n",
    "            plotly.offline.init_notebook_mode()\n",
    "            # Configure the trace.\n",
    "            trace = go.Scatter3d(\n",
    "                x=self.data[0],  # <-- Put your data instead\n",
    "                y=self.data[1],  # <-- Put your data instead\n",
    "                z=self.data[2],  # <-- Put your data instead\n",
    "                mode='markers',\n",
    "                marker=marker\n",
    "            )\n",
    "            # Configure the layout.\n",
    "            layout = go.Layout(\n",
    "                margin={'l': 0, 'r': 0, 'b': 0, 't': 0}\n",
    "            )\n",
    "            data = [trace]\n",
    "            plot_figure = go.Figure(data=data, layout=layout)\n",
    "            # Render the plot.\n",
    "            plotly.offline.iplot(plot_figure)\n",
    "\n",
    "        elif len(self.data.columns) == 2:\n",
    "            self._logger.warning(\"Input dataset contains only 2 features. 2D scatter plot will be created.\")\n",
    "            \n",
    "            # plot 2D scatter plot\n",
    "            fig = go.Figure(data=go.Scatter(\n",
    "                x=self.data[0], \n",
    "                y=self.data[1], \n",
    "                mode='markers', \n",
    "                marker=marker))\n",
    "            fig.show()\n",
    "            return None\n",
    "        else:\n",
    "            self._logger.error(\"Input dataset contains less than 2 features. Insufficient data to plot.\")\n",
    "            return None\n",
    "        \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data object\n",
    "UI_data = data(UI.iloc[:,:2])\n",
    "# perform KMeans clustering to get cluster values\n",
    "UI_data.kmeans(3)\n",
    "# perform GMM clustering to get cluster values\n",
    "UI_data.gmm(n=3,covariance_type=\"full\")\n",
    "# plot data and clusters (colour-coded)\n",
    "UI_data.plotScatter(True, 'kmeans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GR_data = data(GR.iloc[:,:2])\n",
    "GR_data.kmeans(3)\n",
    "GR_data.gmm(n=2,covariance_type=\"full\")\n",
    "GR_data.plotScatter(True,'gmm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRw1_data = data(GRw1.iloc[:,:3])\n",
    "GRw1_data.kmeans(3)\n",
    "GRw1_data.gmm(n=3,covariance_type=\"full\")\n",
    "GRw1_data.plotScatter(True,'kmeans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "GR_data.gmm(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GR = data(dataGR)\n",
    "GR.gmm_eval(n=20,covariance_type=\"full\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRw1 = data(dataGR_w1)\n",
    "GRw1.gmm_eval(n=20,covariance_type=\"full\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine distribution of clusters \n",
    "cluster_count = predictions['cluster'].value_counts()\n",
    "cluster_count.plot.bar(title='Distribution of Users in Clusters (Count)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gaussian Mixture Modelling Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign features and target variable\n",
    "X = UI_temp\n",
    "Y = clustered['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a decision tree to understand the definiing features separating the clusters \n",
    "# and understand how the entropy changes \n",
    "\n",
    "# We split the data into a training (80%) and testing (20%) dataset\n",
    "train_X, test_X, train_Y, test_Y = train_test_split(X, Y, test_size=0.20, random_state=0, stratify=Y)\n",
    "#declare decision tree classifier classifiying based on entropy \n",
    "KMeans_Tree = DecisionTreeClassifier(max_depth =3, criterion ='entropy')\n",
    "#train decision tree classifier on training data\n",
    "KMeans_Tree.fit(train_X, train_Y)\n",
    "#get predicted results for given test_X\n",
    "KMeans_TreePred = KMeans_Tree.predict(test_X)\n",
    "acc_score = metrics.accuracy_score(test_Y, KMeans_TreePred)\n",
    "print(acc_score)\n",
    "#metrics.confusion_matrix(test_Y, KMeans_TreePred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [str(int) for int in range(26)]\n",
    "#Let's visualise the tree\n",
    "tree.export_graphviz(KMeans_Tree, out_file = 'KMeans_Tree.dot', feature_names = X.columns, class_names = [str(int) for int in range(26)], filled = True)\n",
    "# Convert to png\n",
    "from subprocess import call\n",
    "call(['dot', '-Tpng', 'KMeans_Tree.dot', '-o', 'KMeans_Tree.png', '-Gdpi=600'])\n",
    "\n",
    "# Display in python\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize = (14, 18))\n",
    "plt.imshow(plt.imread('KMeans_Tree.png'))\n",
    "plt.axis('off');\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
